* 
* ==> Audit <==
* |---------|-----------------------------|----------|---------------|---------|---------------------|---------------------|
| Command |            Args             | Profile  |     User      | Version |     Start Time      |      End Time       |
|---------|-----------------------------|----------|---------------|---------|---------------------|---------------------|
| start   |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 00:51 +07 | 25 Apr 24 00:52 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 00:52 +07 |                     |
| start   |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 08:36 +07 | 25 Apr 24 08:37 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 08:43 +07 | 25 Apr 24 08:47 +07 |
| service |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 08:47 +07 |                     |
| service | --all                       | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 08:47 +07 | 25 Apr 24 08:48 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 08:57 +07 | 25 Apr 24 09:00 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 09:02 +07 | 25 Apr 24 09:06 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 09:17 +07 | 25 Apr 24 09:19 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 09:36 +07 | 25 Apr 24 09:52 +07 |
| service | minio -n minio              | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 09:50 +07 | 25 Apr 24 09:58 +07 |
| service | minio -n minio              | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 10:04 +07 | 25 Apr 24 10:06 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 10:26 +07 | 25 Apr 24 10:26 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 10:41 +07 | 25 Apr 24 10:42 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 10:44 +07 |                     |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 10:48 +07 | 25 Apr 24 10:49 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 10:49 +07 | 25 Apr 24 10:55 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 10:55 +07 | 25 Apr 24 10:56 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 11:01 +07 | 25 Apr 24 11:03 +07 |
| ip      |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 11:04 +07 | 25 Apr 24 11:04 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 11:08 +07 | 25 Apr 24 11:09 +07 |
| node    |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 11:10 +07 |                     |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 11:13 +07 |                     |
| stop    |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 11:22 +07 | 25 Apr 24 11:22 +07 |
| start   |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 11:23 +07 | 25 Apr 24 11:23 +07 |
| service | minio -n minio              | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 11:41 +07 | 25 Apr 24 11:41 +07 |
| service | minio -n postgres           | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 13:35 +07 | 25 Apr 24 13:35 +07 |
| start   |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 13:39 +07 | 25 Apr 24 13:39 +07 |
| service | minio -n minio-dev          | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 13:43 +07 | 25 Apr 24 13:43 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 13:45 +07 | 25 Apr 24 14:00 +07 |
| service |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 14:05 +07 |                     |
| service | --all                       | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 14:05 +07 | 25 Apr 24 14:05 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 14:05 +07 | 25 Apr 24 14:06 +07 |
| service | minio -n minio-dev          | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 14:14 +07 | 25 Apr 24 14:14 +07 |
| service | minio -n minio-dev          | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 14:22 +07 | 25 Apr 24 14:23 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:00 +07 | 25 Apr 24 15:00 +07 |
| service | postgres --url -n postgres  | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:01 +07 | 25 Apr 24 15:04 +07 |
| tunnel  |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:04 +07 | 25 Apr 24 15:11 +07 |
| stop    |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:12 +07 | 25 Apr 24 15:12 +07 |
| start   |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:13 +07 | 25 Apr 24 15:14 +07 |
| service | minio -n minio              | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:23 +07 | 25 Apr 24 15:28 +07 |
| service | minio -n minio              | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:36 +07 | 25 Apr 24 15:41 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:41 +07 | 25 Apr 24 15:42 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:42 +07 | 25 Apr 24 15:43 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:42 +07 | 25 Apr 24 15:42 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:42 +07 | 25 Apr 24 15:46 +07 |
| service | list                        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:44 +07 | 25 Apr 24 15:44 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:48 +07 | 25 Apr 24 15:48 +07 |
| service | postgres tunnel             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:49 +07 |                     |
| service | postgres tunnel -n postgres | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:49 +07 | 25 Apr 24 15:49 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:50 +07 | 25 Apr 24 15:50 +07 |
| service | list                        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:53 +07 | 25 Apr 24 15:53 +07 |
| service | list                        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 15:54 +07 | 25 Apr 24 15:54 +07 |
| service | list                        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 16:01 +07 | 25 Apr 24 16:01 +07 |
| ip      |                             | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 16:02 +07 | 25 Apr 24 16:02 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 16:06 +07 | 25 Apr 24 16:09 +07 |
| service | list                        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 16:11 +07 | 25 Apr 24 16:11 +07 |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 16:14 +07 |                     |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 16:18 +07 |                     |
| service | postgres -n postgres        | minikube | thh\Huy Hoang | v1.32.0 | 25 Apr 24 16:22 +07 |                     |
|---------|-----------------------------|----------|---------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/04/25 15:13:57
Running on machine: thh
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0425 15:13:57.990657   19004 out.go:296] Setting OutFile to fd 104 ...
I0425 15:13:57.991956   19004 out.go:348] isatty.IsTerminal(104) = true
I0425 15:13:57.991956   19004 out.go:309] Setting ErrFile to fd 108...
I0425 15:13:57.992085   19004 out.go:348] isatty.IsTerminal(108) = true
W0425 15:13:58.003903   19004 root.go:314] Error reading config file at C:\Users\Huy Hoang\.minikube\config\config.json: open C:\Users\Huy Hoang\.minikube\config\config.json: The system cannot find the file specified.
I0425 15:13:58.009934   19004 out.go:303] Setting JSON to false
I0425 15:13:58.016176   19004 start.go:128] hostinfo: {"hostname":"thh","uptime":51800,"bootTime":1713981037,"procs":281,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.3447 Build 22631.3447","kernelVersion":"10.0.22631.3447 Build 22631.3447","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"e010656e-c8ab-43c3-b366-d8cc84b9fa4b"}
W0425 15:13:58.016176   19004 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0425 15:13:58.016739   19004 out.go:177] 😄  minikube v1.32.0 on Microsoft Windows 11 Home Single Language 10.0.22631.3447 Build 22631.3447
I0425 15:13:58.018508   19004 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0425 15:13:58.019022   19004 notify.go:220] Checking for updates...
I0425 15:13:58.020186   19004 driver.go:378] Setting default libvirt URI to qemu:///system
I0425 15:13:58.152332   19004 docker.go:122] docker version: linux-25.0.3:Docker Desktop 4.28.0 (139021)
I0425 15:13:58.154573   19004 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0425 15:13:58.737614   19004 info.go:266] docker info: {ID:d14749a3-7b08-4f53-9d9f-f3e68d76dc1b Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:56 OomKillDisable:true NGoroutines:101 SystemTime:2024-04-25 08:13:58.69752101 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:22 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:6151020544 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I0425 15:13:58.738276   19004 out.go:177] ✨  Using the docker driver based on existing profile
I0425 15:13:58.739039   19004 start.go:298] selected driver: docker
I0425 15:13:58.739039   19004 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Huy Hoang:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0425 15:13:58.739724   19004 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0425 15:13:58.743498   19004 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0425 15:13:59.044838   19004 info.go:266] docker info: {ID:d14749a3-7b08-4f53-9d9f-f3e68d76dc1b Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:56 OomKillDisable:true NGoroutines:101 SystemTime:2024-04-25 08:13:59.002756798 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:22 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:6151020544 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I0425 15:13:59.137451   19004 cni.go:84] Creating CNI manager for ""
I0425 15:13:59.138052   19004 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0425 15:13:59.138052   19004 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Huy Hoang:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0425 15:13:59.139082   19004 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0425 15:13:59.140640   19004 cache.go:121] Beginning downloading kic base image for docker with docker
I0425 15:13:59.140640   19004 out.go:177] 🚜  Pulling base image ...
I0425 15:13:59.141684   19004 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0425 15:13:59.141684   19004 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0425 15:13:59.142205   19004 preload.go:148] Found local preload: C:\Users\Huy Hoang\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0425 15:13:59.142205   19004 cache.go:56] Caching tarball of preloaded images
I0425 15:13:59.142205   19004 preload.go:174] Found C:\Users\Huy Hoang\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0425 15:13:59.142205   19004 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0425 15:13:59.143261   19004 profile.go:148] Saving config to C:\Users\Huy Hoang\.minikube\profiles\minikube\config.json ...
I0425 15:13:59.261737   19004 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0425 15:13:59.261737   19004 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0425 15:13:59.262319   19004 cache.go:194] Successfully downloaded all kic artifacts
I0425 15:13:59.262386   19004 start.go:365] acquiring machines lock for minikube: {Name:mk82143f5b4ea499b90589be0b8b8dd2e10fd3b9 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0425 15:13:59.262386   19004 start.go:369] acquired machines lock for "minikube" in 0s
I0425 15:13:59.262386   19004 start.go:96] Skipping create...Using existing machine configuration
I0425 15:13:59.262893   19004 fix.go:54] fixHost starting: 
I0425 15:13:59.266314   19004 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0425 15:13:59.384382   19004 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0425 15:13:59.384382   19004 fix.go:128] unexpected machine state, will restart: <nil>
I0425 15:13:59.384964   19004 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0425 15:13:59.387293   19004 cli_runner.go:164] Run: docker start minikube
I0425 15:14:00.131656   19004 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0425 15:14:00.284838   19004 kic.go:430] container "minikube" state is running.
I0425 15:14:00.292840   19004 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0425 15:14:00.748984   19004 profile.go:148] Saving config to C:\Users\Huy Hoang\.minikube\profiles\minikube\config.json ...
I0425 15:14:00.751441   19004 machine.go:88] provisioning docker machine ...
I0425 15:14:00.757532   19004 ubuntu.go:169] provisioning hostname "minikube"
I0425 15:14:00.767386   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:00.936219   19004 main.go:141] libmachine: Using SSH client type: native
I0425 15:14:00.936740   19004 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9447e0] 0x947320 <nil>  [] 0s} 127.0.0.1 61934 <nil> <nil>}
I0425 15:14:00.936740   19004 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0425 15:14:01.099288   19004 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0425 15:14:01.103672   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:01.275763   19004 main.go:141] libmachine: Using SSH client type: native
I0425 15:14:01.276356   19004 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9447e0] 0x947320 <nil>  [] 0s} 127.0.0.1 61934 <nil> <nil>}
I0425 15:14:01.276356   19004 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0425 15:14:01.443206   19004 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0425 15:14:01.444147   19004 ubuntu.go:175] set auth options {CertDir:C:\Users\Huy Hoang\.minikube CaCertPath:C:\Users\Huy Hoang\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Huy Hoang\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Huy Hoang\.minikube\machines\server.pem ServerKeyPath:C:\Users\Huy Hoang\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Huy Hoang\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Huy Hoang\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Huy Hoang\.minikube}
I0425 15:14:01.444655   19004 ubuntu.go:177] setting up certificates
I0425 15:14:01.444655   19004 provision.go:83] configureAuth start
I0425 15:14:01.451889   19004 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0425 15:14:01.599281   19004 provision.go:138] copyHostCerts
I0425 15:14:01.599929   19004 exec_runner.go:144] found C:\Users\Huy Hoang\.minikube/ca.pem, removing ...
I0425 15:14:01.599929   19004 exec_runner.go:203] rm: C:\Users\Huy Hoang\.minikube\ca.pem
I0425 15:14:01.599929   19004 exec_runner.go:151] cp: C:\Users\Huy Hoang\.minikube\certs\ca.pem --> C:\Users\Huy Hoang\.minikube/ca.pem (1086 bytes)
I0425 15:14:01.600976   19004 exec_runner.go:144] found C:\Users\Huy Hoang\.minikube/cert.pem, removing ...
I0425 15:14:01.600976   19004 exec_runner.go:203] rm: C:\Users\Huy Hoang\.minikube\cert.pem
I0425 15:14:01.601514   19004 exec_runner.go:151] cp: C:\Users\Huy Hoang\.minikube\certs\cert.pem --> C:\Users\Huy Hoang\.minikube/cert.pem (1131 bytes)
I0425 15:14:01.602557   19004 exec_runner.go:144] found C:\Users\Huy Hoang\.minikube/key.pem, removing ...
I0425 15:14:01.602557   19004 exec_runner.go:203] rm: C:\Users\Huy Hoang\.minikube\key.pem
I0425 15:14:01.602557   19004 exec_runner.go:151] cp: C:\Users\Huy Hoang\.minikube\certs\key.pem --> C:\Users\Huy Hoang\.minikube/key.pem (1675 bytes)
I0425 15:14:01.603623   19004 provision.go:112] generating server cert: C:\Users\Huy Hoang\.minikube\machines\server.pem ca-key=C:\Users\Huy Hoang\.minikube\certs\ca.pem private-key=C:\Users\Huy Hoang\.minikube\certs\ca-key.pem org=Huy Hoang.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0425 15:14:01.829413   19004 provision.go:172] copyRemoteCerts
I0425 15:14:01.833550   19004 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0425 15:14:01.836153   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:01.956670   19004 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61934 SSHKeyPath:C:\Users\Huy Hoang\.minikube\machines\minikube\id_rsa Username:docker}
I0425 15:14:02.048961   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0425 15:14:02.071293   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\machines\server.pem --> /etc/docker/server.pem (1208 bytes)
I0425 15:14:02.092262   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0425 15:14:02.122696   19004 provision.go:86] duration metric: configureAuth took 678.0419ms
I0425 15:14:02.122696   19004 ubuntu.go:193] setting minikube options for container-runtime
I0425 15:14:02.122696   19004 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0425 15:14:02.124290   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:02.249257   19004 main.go:141] libmachine: Using SSH client type: native
I0425 15:14:02.249772   19004 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9447e0] 0x947320 <nil>  [] 0s} 127.0.0.1 61934 <nil> <nil>}
I0425 15:14:02.249772   19004 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0425 15:14:02.385644   19004 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0425 15:14:02.385644   19004 ubuntu.go:71] root file system type: overlay
I0425 15:14:02.385893   19004 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0425 15:14:02.387997   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:02.557963   19004 main.go:141] libmachine: Using SSH client type: native
I0425 15:14:02.557963   19004 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9447e0] 0x947320 <nil>  [] 0s} 127.0.0.1 61934 <nil> <nil>}
I0425 15:14:02.557963   19004 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0425 15:14:02.733441   19004 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0425 15:14:02.735565   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:02.866245   19004 main.go:141] libmachine: Using SSH client type: native
I0425 15:14:02.866751   19004 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9447e0] 0x947320 <nil>  [] 0s} 127.0.0.1 61934 <nil> <nil>}
I0425 15:14:02.866751   19004 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0425 15:14:03.002273   19004 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0425 15:14:03.002273   19004 machine.go:91] provisioned docker machine in 2.2508326s
I0425 15:14:03.002273   19004 start.go:300] post-start starting for "minikube" (driver="docker")
I0425 15:14:03.002273   19004 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0425 15:14:03.005537   19004 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0425 15:14:03.006674   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:03.156045   19004 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61934 SSHKeyPath:C:\Users\Huy Hoang\.minikube\machines\minikube\id_rsa Username:docker}
I0425 15:14:03.263932   19004 ssh_runner.go:195] Run: cat /etc/os-release
I0425 15:14:03.269956   19004 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0425 15:14:03.269956   19004 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0425 15:14:03.269956   19004 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0425 15:14:03.269956   19004 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0425 15:14:03.269956   19004 filesync.go:126] Scanning C:\Users\Huy Hoang\.minikube\addons for local assets ...
I0425 15:14:03.270482   19004 filesync.go:126] Scanning C:\Users\Huy Hoang\.minikube\files for local assets ...
I0425 15:14:03.270482   19004 start.go:303] post-start completed in 268.2091ms
I0425 15:14:03.275217   19004 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0425 15:14:03.277422   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:03.416960   19004 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61934 SSHKeyPath:C:\Users\Huy Hoang\.minikube\machines\minikube\id_rsa Username:docker}
I0425 15:14:03.510975   19004 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0425 15:14:03.517406   19004 fix.go:56] fixHost completed within 4.2550197s
I0425 15:14:03.517406   19004 start.go:83] releasing machines lock for "minikube", held for 4.2550197s
I0425 15:14:03.519545   19004 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0425 15:14:03.662976   19004 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0425 15:14:03.666165   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:03.666709   19004 ssh_runner.go:195] Run: cat /version.json
I0425 15:14:03.668377   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:03.971710   19004 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61934 SSHKeyPath:C:\Users\Huy Hoang\.minikube\machines\minikube\id_rsa Username:docker}
I0425 15:14:03.984745   19004 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61934 SSHKeyPath:C:\Users\Huy Hoang\.minikube\machines\minikube\id_rsa Username:docker}
I0425 15:14:04.548002   19004 ssh_runner.go:195] Run: systemctl --version
I0425 15:14:04.556688   19004 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0425 15:14:04.565475   19004 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0425 15:14:04.574271   19004 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0425 15:14:04.577475   19004 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0425 15:14:04.587518   19004 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0425 15:14:04.587518   19004 start.go:472] detecting cgroup driver to use...
I0425 15:14:04.587518   19004 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0425 15:14:04.589136   19004 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0425 15:14:04.607221   19004 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0425 15:14:04.620611   19004 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0425 15:14:04.631220   19004 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0425 15:14:04.633937   19004 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0425 15:14:04.646572   19004 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0425 15:14:04.660153   19004 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0425 15:14:04.673248   19004 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0425 15:14:04.688040   19004 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0425 15:14:04.702608   19004 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0425 15:14:04.716583   19004 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0425 15:14:04.728510   19004 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0425 15:14:04.740719   19004 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0425 15:14:04.814299   19004 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0425 15:14:04.929834   19004 start.go:472] detecting cgroup driver to use...
I0425 15:14:04.929834   19004 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0425 15:14:04.936977   19004 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0425 15:14:04.952322   19004 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0425 15:14:04.955510   19004 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0425 15:14:04.968385   19004 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0425 15:14:04.989201   19004 ssh_runner.go:195] Run: which cri-dockerd
I0425 15:14:04.998961   19004 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0425 15:14:05.009215   19004 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0425 15:14:05.037771   19004 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0425 15:14:05.653826   19004 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0425 15:14:05.871025   19004 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0425 15:14:05.871025   19004 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0425 15:14:05.916951   19004 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0425 15:14:06.170024   19004 ssh_runner.go:195] Run: sudo systemctl restart docker
I0425 15:14:07.122030   19004 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0425 15:14:07.211683   19004 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0425 15:14:07.342363   19004 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0425 15:14:07.467495   19004 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0425 15:14:07.601419   19004 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0425 15:14:07.626274   19004 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0425 15:14:07.754284   19004 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0425 15:14:07.856603   19004 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0425 15:14:07.861519   19004 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0425 15:14:07.867353   19004 start.go:540] Will wait 60s for crictl version
I0425 15:14:07.871108   19004 ssh_runner.go:195] Run: which crictl
I0425 15:14:07.879951   19004 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0425 15:14:07.942978   19004 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0425 15:14:07.945672   19004 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0425 15:14:07.979442   19004 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0425 15:14:08.010947   19004 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0425 15:14:08.013591   19004 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0425 15:14:08.272705   19004 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0425 15:14:08.277564   19004 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0425 15:14:08.285186   19004 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0425 15:14:08.306545   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0425 15:14:08.504587   19004 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0425 15:14:08.506743   19004 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0425 15:14:08.543671   19004 docker.go:671] Got preloaded images: -- stdout --
quay.io/minio/minio:latest
minio/minio:latest
postgres:14
postgres:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5
minio/minio:RELEASE.2017-02-16T01-47-30Z

-- /stdout --
I0425 15:14:08.544203   19004 docker.go:601] Images already preloaded, skipping extraction
I0425 15:14:08.546933   19004 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0425 15:14:08.580490   19004 docker.go:671] Got preloaded images: -- stdout --
minio/minio:latest
quay.io/minio/minio:latest
postgres:latest
postgres:14
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5
minio/minio:RELEASE.2017-02-16T01-47-30Z

-- /stdout --
I0425 15:14:08.580490   19004 cache_images.go:84] Images are preloaded, skipping loading
I0425 15:14:08.586131   19004 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0425 15:14:08.689841   19004 cni.go:84] Creating CNI manager for ""
I0425 15:14:08.692014   19004 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0425 15:14:08.692014   19004 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0425 15:14:08.692014   19004 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0425 15:14:08.692547   19004 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0425 15:14:08.692547   19004 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0425 15:14:08.699667   19004 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0425 15:14:08.717206   19004 binaries.go:44] Found k8s binaries, skipping transfer
I0425 15:14:08.726420   19004 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0425 15:14:08.752127   19004 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0425 15:14:08.788505   19004 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0425 15:14:08.839952   19004 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0425 15:14:08.891359   19004 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0425 15:14:08.921375   19004 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0425 15:14:08.947051   19004 certs.go:56] Setting up C:\Users\Huy Hoang\.minikube\profiles\minikube for IP: 192.168.49.2
I0425 15:14:08.948395   19004 certs.go:190] acquiring lock for shared ca certs: {Name:mkca1d4c1d7e6ce83b8974c171b84b9e4431e9de Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0425 15:14:08.950045   19004 certs.go:199] skipping minikubeCA CA generation: C:\Users\Huy Hoang\.minikube\ca.key
I0425 15:14:08.951127   19004 certs.go:199] skipping proxyClientCA CA generation: C:\Users\Huy Hoang\.minikube\proxy-client-ca.key
I0425 15:14:08.952766   19004 certs.go:315] skipping minikube-user signed cert generation: C:\Users\Huy Hoang\.minikube\profiles\minikube\client.key
I0425 15:14:08.954545   19004 certs.go:315] skipping minikube signed cert generation: C:\Users\Huy Hoang\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0425 15:14:08.956177   19004 certs.go:315] skipping aggregator signed cert generation: C:\Users\Huy Hoang\.minikube\profiles\minikube\proxy-client.key
I0425 15:14:08.959514   19004 certs.go:437] found cert: C:\Users\Huy Hoang\.minikube\certs\C:\Users\Huy Hoang\.minikube\certs\ca-key.pem (1679 bytes)
I0425 15:14:08.960068   19004 certs.go:437] found cert: C:\Users\Huy Hoang\.minikube\certs\C:\Users\Huy Hoang\.minikube\certs\ca.pem (1086 bytes)
I0425 15:14:08.960068   19004 certs.go:437] found cert: C:\Users\Huy Hoang\.minikube\certs\C:\Users\Huy Hoang\.minikube\certs\cert.pem (1131 bytes)
I0425 15:14:08.961756   19004 certs.go:437] found cert: C:\Users\Huy Hoang\.minikube\certs\C:\Users\Huy Hoang\.minikube\certs\key.pem (1675 bytes)
I0425 15:14:08.967971   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0425 15:14:09.027894   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0425 15:14:09.094874   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0425 15:14:09.153112   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0425 15:14:09.195350   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0425 15:14:09.248358   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0425 15:14:09.289859   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0425 15:14:09.338663   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0425 15:14:09.383527   19004 ssh_runner.go:362] scp C:\Users\Huy Hoang\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0425 15:14:09.437737   19004 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0425 15:14:09.480339   19004 ssh_runner.go:195] Run: openssl version
I0425 15:14:09.502320   19004 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0425 15:14:09.529299   19004 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0425 15:14:09.539828   19004 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Apr 19 10:22 /usr/share/ca-certificates/minikubeCA.pem
I0425 15:14:09.546474   19004 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0425 15:14:09.571360   19004 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0425 15:14:09.597139   19004 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0425 15:14:09.614183   19004 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0425 15:14:09.638033   19004 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0425 15:14:09.658933   19004 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0425 15:14:09.679062   19004 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0425 15:14:09.699239   19004 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0425 15:14:09.722932   19004 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0425 15:14:09.735701   19004 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Huy Hoang:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0425 15:14:09.740224   19004 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0425 15:14:09.785250   19004 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0425 15:14:09.801780   19004 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0425 15:14:09.801780   19004 kubeadm.go:636] restartCluster start
I0425 15:14:09.809417   19004 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0425 15:14:09.825793   19004 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0425 15:14:09.829055   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0425 15:14:10.016377   19004 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in C:\Users\Huy Hoang\.kube\config
I0425 15:14:10.016922   19004 kubeconfig.go:146] "minikube" context is missing from C:\Users\Huy Hoang\.kube\config - will repair!
I0425 15:14:10.017467   19004 lock.go:35] WriteFile acquiring C:\Users\Huy Hoang\.kube\config: {Name:mk3fade1ad797bfd10e1cefa58cdb1ea8e629a91 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0425 15:14:10.043438   19004 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0425 15:14:10.059740   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:10.067060   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:10.084071   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:10.084071   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:10.090096   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:10.106672   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:10.613147   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:10.618761   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:10.633569   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:11.113792   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:11.118017   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:11.130170   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:11.614685   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:11.618756   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:11.630481   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:12.120850   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:12.123556   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:12.135108   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:12.609828   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:12.614119   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:12.626152   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:13.117514   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:13.122537   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:13.143146   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:13.610230   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:13.613939   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:13.626213   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:14.109357   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:14.112762   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:14.127272   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:14.608576   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:14.616788   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:14.635477   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:15.113842   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:15.125657   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:15.166729   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:15.616595   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:15.634619   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:15.681682   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:16.111986   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:16.124015   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:16.151493   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:16.614361   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:16.621300   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:16.641370   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:17.110911   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:17.118610   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:17.145616   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:17.614014   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:17.621245   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:17.641001   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:18.115118   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:18.125594   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:18.147900   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:18.623069   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:18.629057   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:18.646982   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:19.117899   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:19.122259   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:19.135089   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:19.616958   19004 api_server.go:166] Checking apiserver status ...
I0425 15:14:19.620105   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0425 15:14:19.629538   19004 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0425 15:14:20.068577   19004 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0425 15:14:20.068644   19004 kubeadm.go:1128] stopping kube-system containers ...
I0425 15:14:20.070765   19004 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0425 15:14:20.095190   19004 docker.go:469] Stopping containers: [b0b57631b62e 080c3213aa40 5638ae6808a9 737ac849a686 75a40ef3aac6 874f1c8965d6 9922e13294b2 11618bb335aa bf3d2912c1de 0f553fd8bdb9 57f6ab278e73 35d496e64888 dd5b1a15d791 b6413662122d 9f9448178b5c 534ec9f405e4 8132af5312ba bcb3f5c404d1 e951ff8e2e07 34795f6e0be5 aab63875be45 67cca1d173cb 0e5cf169bcad a1bd8ef0598a 4adfa2045863 726a0987ac48 a4ddab87f54e]
I0425 15:14:20.097289   19004 ssh_runner.go:195] Run: docker stop b0b57631b62e 080c3213aa40 5638ae6808a9 737ac849a686 75a40ef3aac6 874f1c8965d6 9922e13294b2 11618bb335aa bf3d2912c1de 0f553fd8bdb9 57f6ab278e73 35d496e64888 dd5b1a15d791 b6413662122d 9f9448178b5c 534ec9f405e4 8132af5312ba bcb3f5c404d1 e951ff8e2e07 34795f6e0be5 aab63875be45 67cca1d173cb 0e5cf169bcad a1bd8ef0598a 4adfa2045863 726a0987ac48 a4ddab87f54e
I0425 15:14:20.128985   19004 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0425 15:14:20.147223   19004 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0425 15:14:20.158612   19004 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Apr 19 10:22 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Apr 25 04:23 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Apr 19 10:22 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Apr 25 04:23 /etc/kubernetes/scheduler.conf

I0425 15:14:20.161853   19004 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0425 15:14:20.177398   19004 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0425 15:14:20.192232   19004 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0425 15:14:20.204174   19004 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0425 15:14:20.209508   19004 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0425 15:14:20.228424   19004 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0425 15:14:20.242243   19004 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0425 15:14:20.246752   19004 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0425 15:14:20.263066   19004 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0425 15:14:20.276284   19004 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0425 15:14:20.276284   19004 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0425 15:14:20.564375   19004 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0425 15:14:21.500800   19004 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0425 15:14:21.782764   19004 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0425 15:14:21.872802   19004 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0425 15:14:21.957938   19004 api_server.go:52] waiting for apiserver process to appear ...
I0425 15:14:21.963337   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:21.984189   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:22.505026   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:23.010375   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:23.522808   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:24.018024   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:24.523947   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:25.017661   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:25.513241   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:26.016937   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:26.509446   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:27.008971   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:27.512888   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:28.007856   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:28.507540   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:29.014806   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:29.513678   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:30.009752   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:30.050924   19004 api_server.go:72] duration metric: took 8.0929863s to wait for apiserver process to appear ...
I0425 15:14:30.051211   19004 api_server.go:88] waiting for apiserver healthz status ...
I0425 15:14:30.051211   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:30.059575   19004 api_server.go:269] stopped: https://127.0.0.1:61938/healthz: Get "https://127.0.0.1:61938/healthz": EOF
I0425 15:14:30.059575   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:30.062241   19004 api_server.go:269] stopped: https://127.0.0.1:61938/healthz: Get "https://127.0.0.1:61938/healthz": EOF
I0425 15:14:30.566282   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:30.570640   19004 api_server.go:269] stopped: https://127.0.0.1:61938/healthz: Get "https://127.0.0.1:61938/healthz": EOF
I0425 15:14:31.071671   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:31.074870   19004 api_server.go:269] stopped: https://127.0.0.1:61938/healthz: Get "https://127.0.0.1:61938/healthz": EOF
I0425 15:14:31.567035   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:36.434473   19004 api_server.go:279] https://127.0.0.1:61938/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0425 15:14:36.434991   19004 api_server.go:103] status: https://127.0.0.1:61938/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0425 15:14:36.434991   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:36.634000   19004 api_server.go:279] https://127.0.0.1:61938/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0425 15:14:36.634000   19004 api_server.go:103] status: https://127.0.0.1:61938/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0425 15:14:36.634000   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:36.743494   19004 api_server.go:279] https://127.0.0.1:61938/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0425 15:14:36.743494   19004 api_server.go:103] status: https://127.0.0.1:61938/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0425 15:14:37.062995   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:37.079412   19004 api_server.go:279] https://127.0.0.1:61938/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0425 15:14:37.079412   19004 api_server.go:103] status: https://127.0.0.1:61938/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0425 15:14:37.570614   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:37.581882   19004 api_server.go:279] https://127.0.0.1:61938/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0425 15:14:37.581882   19004 api_server.go:103] status: https://127.0.0.1:61938/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0425 15:14:38.066075   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:38.134319   19004 api_server.go:279] https://127.0.0.1:61938/healthz returned 200:
ok
I0425 15:14:38.157248   19004 api_server.go:141] control plane version: v1.28.3
I0425 15:14:38.157248   19004 api_server.go:131] duration metric: took 8.1060371s to wait for apiserver health ...
I0425 15:14:38.157248   19004 cni.go:84] Creating CNI manager for ""
I0425 15:14:38.157248   19004 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0425 15:14:38.157787   19004 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0425 15:14:38.165496   19004 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0425 15:14:38.231801   19004 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0425 15:14:38.336282   19004 system_pods.go:43] waiting for kube-system pods to appear ...
I0425 15:14:38.362768   19004 system_pods.go:59] 7 kube-system pods found
I0425 15:14:38.362768   19004 system_pods.go:61] "coredns-5dd5756b68-hlhl8" [29848126-54b5-4ba7-a62c-9cc3ff6f45c4] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0425 15:14:38.362768   19004 system_pods.go:61] "etcd-minikube" [63618ac5-f5c8-42c3-9b73-26ed420ac540] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0425 15:14:38.362768   19004 system_pods.go:61] "kube-apiserver-minikube" [7531510a-9eee-4cff-904a-25a3beadb1af] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0425 15:14:38.362768   19004 system_pods.go:61] "kube-controller-manager-minikube" [7e562b2e-1db6-436c-867a-f20a36ba2e39] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0425 15:14:38.362768   19004 system_pods.go:61] "kube-proxy-nr5vr" [2212206f-1b60-4bbd-a29a-ecc752fba973] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0425 15:14:38.362768   19004 system_pods.go:61] "kube-scheduler-minikube" [c67379b7-2149-4645-9546-f57ca54bad5e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0425 15:14:38.362768   19004 system_pods.go:61] "storage-provisioner" [b5c241cd-ce27-496e-a91f-a16aa1bf0b35] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0425 15:14:38.362768   19004 system_pods.go:74] duration metric: took 26.4859ms to wait for pod list to return data ...
I0425 15:14:38.362768   19004 node_conditions.go:102] verifying NodePressure condition ...
I0425 15:14:38.368953   19004 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0425 15:14:38.368953   19004 node_conditions.go:123] node cpu capacity is 8
I0425 15:14:38.369464   19004 node_conditions.go:105] duration metric: took 6.6959ms to run NodePressure ...
I0425 15:14:38.369464   19004 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0425 15:14:39.093641   19004 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0425 15:14:39.122347   19004 ops.go:34] apiserver oom_adj: -16
I0425 15:14:39.122853   19004 kubeadm.go:640] restartCluster took 29.3205668s
I0425 15:14:39.122853   19004 kubeadm.go:406] StartCluster complete in 29.3871522s
I0425 15:14:39.123376   19004 settings.go:142] acquiring lock: {Name:mk587e65fbec7455572cb8bc1dfcbc3c2b3c1fb2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0425 15:14:39.123376   19004 settings.go:150] Updating kubeconfig:  C:\Users\Huy Hoang\.kube\config
I0425 15:14:39.123984   19004 lock.go:35] WriteFile acquiring C:\Users\Huy Hoang\.kube\config: {Name:mk3fade1ad797bfd10e1cefa58cdb1ea8e629a91 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0425 15:14:39.125018   19004 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0425 15:14:39.125958   19004 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0425 15:14:39.126491   19004 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0425 15:14:39.127101   19004 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0425 15:14:39.127101   19004 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0425 15:14:39.127101   19004 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0425 15:14:39.127101   19004 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0425 15:14:39.127101   19004 addons.go:240] addon storage-provisioner should already be in state true
I0425 15:14:39.127101   19004 host.go:66] Checking if "minikube" exists ...
I0425 15:14:39.134663   19004 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0425 15:14:39.135225   19004 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0425 15:14:39.155838   19004 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0425 15:14:39.156377   19004 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0425 15:14:39.156900   19004 out.go:177] 🔎  Verifying Kubernetes components...
I0425 15:14:39.162060   19004 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0425 15:14:39.291190   19004 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0425 15:14:39.291697   19004 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0425 15:14:39.291697   19004 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0425 15:14:39.292233   19004 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0425 15:14:39.292233   19004 addons.go:240] addon default-storageclass should already be in state true
I0425 15:14:39.292479   19004 host.go:66] Checking if "minikube" exists ...
I0425 15:14:39.294120   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:39.296773   19004 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0425 15:14:39.444379   19004 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0425 15:14:39.444379   19004 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0425 15:14:39.447008   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0425 15:14:39.460029   19004 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61934 SSHKeyPath:C:\Users\Huy Hoang\.minikube\machines\minikube\id_rsa Username:docker}
I0425 15:14:39.480366   19004 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0425 15:14:39.483346   19004 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0425 15:14:39.579706   19004 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0425 15:14:39.597106   19004 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61934 SSHKeyPath:C:\Users\Huy Hoang\.minikube\machines\minikube\id_rsa Username:docker}
I0425 15:14:39.622735   19004 api_server.go:52] waiting for apiserver process to appear ...
I0425 15:14:39.627521   19004 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0425 15:14:39.752723   19004 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0425 15:14:40.650282   19004 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.0705761s)
I0425 15:14:40.650282   19004 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.0227607s)
I0425 15:14:40.650282   19004 api_server.go:72] duration metric: took 1.4939047s to wait for apiserver process to appear ...
I0425 15:14:40.650282   19004 api_server.go:88] waiting for apiserver healthz status ...
I0425 15:14:40.650282   19004 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61938/healthz ...
I0425 15:14:40.662041   19004 api_server.go:279] https://127.0.0.1:61938/healthz returned 200:
ok
I0425 15:14:40.663117   19004 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0425 15:14:40.663653   19004 addons.go:502] enable addons completed in 1.5380932s: enabled=[storage-provisioner default-storageclass]
I0425 15:14:40.664198   19004 api_server.go:141] control plane version: v1.28.3
I0425 15:14:40.664198   19004 api_server.go:131] duration metric: took 13.9161ms to wait for apiserver health ...
I0425 15:14:40.664198   19004 system_pods.go:43] waiting for kube-system pods to appear ...
I0425 15:14:40.672966   19004 system_pods.go:59] 7 kube-system pods found
I0425 15:14:40.672966   19004 system_pods.go:61] "coredns-5dd5756b68-hlhl8" [29848126-54b5-4ba7-a62c-9cc3ff6f45c4] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0425 15:14:40.672966   19004 system_pods.go:61] "etcd-minikube" [63618ac5-f5c8-42c3-9b73-26ed420ac540] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0425 15:14:40.672966   19004 system_pods.go:61] "kube-apiserver-minikube" [7531510a-9eee-4cff-904a-25a3beadb1af] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0425 15:14:40.672966   19004 system_pods.go:61] "kube-controller-manager-minikube" [7e562b2e-1db6-436c-867a-f20a36ba2e39] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0425 15:14:40.672966   19004 system_pods.go:61] "kube-proxy-nr5vr" [2212206f-1b60-4bbd-a29a-ecc752fba973] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0425 15:14:40.672966   19004 system_pods.go:61] "kube-scheduler-minikube" [c67379b7-2149-4645-9546-f57ca54bad5e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0425 15:14:40.672966   19004 system_pods.go:61] "storage-provisioner" [b5c241cd-ce27-496e-a91f-a16aa1bf0b35] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0425 15:14:40.672966   19004 system_pods.go:74] duration metric: took 8.7683ms to wait for pod list to return data ...
I0425 15:14:40.672966   19004 kubeadm.go:581] duration metric: took 1.5165891s to wait for : map[apiserver:true system_pods:true] ...
I0425 15:14:40.672966   19004 node_conditions.go:102] verifying NodePressure condition ...
I0425 15:14:40.678649   19004 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0425 15:14:40.678649   19004 node_conditions.go:123] node cpu capacity is 8
I0425 15:14:40.678649   19004 node_conditions.go:105] duration metric: took 5.6826ms to run NodePressure ...
I0425 15:14:40.678649   19004 start.go:228] waiting for startup goroutines ...
I0425 15:14:40.678649   19004 start.go:233] waiting for cluster config update ...
I0425 15:14:40.678649   19004 start.go:242] writing updated cluster config ...
I0425 15:14:40.688704   19004 ssh_runner.go:195] Run: rm -f paused
I0425 15:14:40.815335   19004 start.go:600] kubectl: 1.29.1, cluster: 1.28.3 (minor skew: 1)
I0425 15:14:40.816086   19004 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Apr 25 08:14:07 minikube dockerd[940]: time="2024-04-25T08:14:07.035084244Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Apr 25 08:14:07 minikube dockerd[940]: time="2024-04-25T08:14:07.035180444Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Apr 25 08:14:07 minikube dockerd[940]: time="2024-04-25T08:14:07.035195644Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Apr 25 08:14:07 minikube dockerd[940]: time="2024-04-25T08:14:07.035204844Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Apr 25 08:14:07 minikube dockerd[940]: time="2024-04-25T08:14:07.035234044Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Apr 25 08:14:07 minikube dockerd[940]: time="2024-04-25T08:14:07.035306144Z" level=info msg="Daemon has completed initialization"
Apr 25 08:14:07 minikube dockerd[940]: time="2024-04-25T08:14:07.114280865Z" level=info msg="API listen on /var/run/docker.sock"
Apr 25 08:14:07 minikube dockerd[940]: time="2024-04-25T08:14:07.114334466Z" level=info msg="API listen on [::]:2376"
Apr 25 08:14:07 minikube systemd[1]: Started Docker Application Container Engine.
Apr 25 08:14:07 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Apr 25 08:14:07 minikube cri-dockerd[1179]: time="2024-04-25T08:14:07Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Apr 25 08:14:07 minikube cri-dockerd[1179]: time="2024-04-25T08:14:07Z" level=info msg="Start docker client with request timeout 0s"
Apr 25 08:14:07 minikube cri-dockerd[1179]: time="2024-04-25T08:14:07Z" level=info msg="Hairpin mode is set to hairpin-veth"
Apr 25 08:14:07 minikube cri-dockerd[1179]: time="2024-04-25T08:14:07Z" level=info msg="Loaded network plugin cni"
Apr 25 08:14:07 minikube cri-dockerd[1179]: time="2024-04-25T08:14:07Z" level=info msg="Docker cri networking managed by network plugin cni"
Apr 25 08:14:07 minikube cri-dockerd[1179]: time="2024-04-25T08:14:07Z" level=info msg="Docker Info: &{ID:7e2dc816-68e1-467d-870a-331a8c644be6 Containers:31 ContainersRunning:0 ContainersPaused:0 ContainersStopped:31 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:35 SystemTime:2024-04-25T08:14:07.844593112Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0006b97a0 NCPU:8 MemTotal:6151020544 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Apr 25 08:14:07 minikube cri-dockerd[1179]: time="2024-04-25T08:14:07Z" level=info msg="Setting cgroupDriver cgroupfs"
Apr 25 08:14:07 minikube cri-dockerd[1179]: time="2024-04-25T08:14:07Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Apr 25 08:14:07 minikube cri-dockerd[1179]: time="2024-04-25T08:14:07Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Apr 25 08:14:07 minikube cri-dockerd[1179]: time="2024-04-25T08:14:07Z" level=info msg="Start cri-dockerd grpc backend"
Apr 25 08:14:07 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Apr 25 08:14:22 minikube cri-dockerd[1179]: time="2024-04-25T08:14:22Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-hlhl8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"bf3d2912c1deb6440cd28fa06a6c820646025f56ecb3e09640b416303f180538\""
Apr 25 08:14:22 minikube cri-dockerd[1179]: time="2024-04-25T08:14:22Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-hlhl8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"bcb3f5c404d12635daec678de7fc21a57fb778799d0c63337c707a2f7fca03b1\""
Apr 25 08:14:22 minikube cri-dockerd[1179]: time="2024-04-25T08:14:22Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"postgres-0_postgres\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"886e5fc7fa4f9644b7718040761ecf6abe0cfec79d0ff1ba742e6d1d92d290c6\""
Apr 25 08:14:22 minikube cri-dockerd[1179]: time="2024-04-25T08:14:22Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"postgres-0_postgres\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e5c831d214f04dbb3a753924c55acdcefaac0caaebace620e118ddbde1cb2a3f\""
Apr 25 08:14:22 minikube cri-dockerd[1179]: time="2024-04-25T08:14:22Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"4adfa2045863c3e3a6c65b2ef543f5952e111dd8e3c9e3a02492f50bf76d433a\". Proceed without further sandbox information."
Apr 25 08:14:23 minikube cri-dockerd[1179]: time="2024-04-25T08:14:23Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a1bd8ef0598a407895cea5a5ca9121ee279e2c3beed55b5b3251b0cb143ec5e6\". Proceed without further sandbox information."
Apr 25 08:14:23 minikube cri-dockerd[1179]: time="2024-04-25T08:14:23Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"726a0987ac480b3bd763a8d8f234c146c6a6df187ac2a59def500883e7c9b8aa\". Proceed without further sandbox information."
Apr 25 08:14:23 minikube cri-dockerd[1179]: time="2024-04-25T08:14:23Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a4ddab87f54e51d595c0518afdc6821a345b021797e02862647acc7d3fb748f9\". Proceed without further sandbox information."
Apr 25 08:14:23 minikube cri-dockerd[1179]: time="2024-04-25T08:14:23Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"postgres-0_postgres\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"886e5fc7fa4f9644b7718040761ecf6abe0cfec79d0ff1ba742e6d1d92d290c6\""
Apr 25 08:14:23 minikube cri-dockerd[1179]: time="2024-04-25T08:14:23Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-hlhl8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"bf3d2912c1deb6440cd28fa06a6c820646025f56ecb3e09640b416303f180538\""
Apr 25 08:14:25 minikube cri-dockerd[1179]: time="2024-04-25T08:14:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5fe50bffa5a75601a6d20d7a81c45b9d5c399fabe97b96d7f53268e497c0819b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 25 08:14:25 minikube cri-dockerd[1179]: time="2024-04-25T08:14:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b3df6726e86ae51faa32b7a069df38ed553308c5152db0058caf3ea54015f180/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 25 08:14:25 minikube cri-dockerd[1179]: time="2024-04-25T08:14:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/582f589efd36106524e5ab7cdc1f2f3c6f7fd3d1e46dc22a918b6856705fd347/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 25 08:14:25 minikube cri-dockerd[1179]: time="2024-04-25T08:14:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/851c1dbc39e9e756fe3508841e58a4f41bdf71da6aa34020d8100b03ae36dbc5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 25 08:14:36 minikube cri-dockerd[1179]: time="2024-04-25T08:14:36Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Apr 25 08:14:38 minikube cri-dockerd[1179]: time="2024-04-25T08:14:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/79939aedd318bcabc9dae4644c407f681967d2a5672a6de3397723fae141d1d2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 25 08:14:38 minikube cri-dockerd[1179]: time="2024-04-25T08:14:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9d9cab24d727a94a764f1342e9814718ee1001cb0f46e91ab9bdb41b22b22cdc/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 25 08:14:38 minikube cri-dockerd[1179]: time="2024-04-25T08:14:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/708a2fd49a80d6842a1d9b6cc2bcf0d844a8407f7133e97753724b580767d7ef/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 25 08:14:38 minikube cri-dockerd[1179]: time="2024-04-25T08:14:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0e23184f3c8828ebe8c0674f2bb5768abba41d651f4643913e73e1e23d041674/resolv.conf as [nameserver 10.96.0.10 search postgres.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 25 08:17:50 minikube cri-dockerd[1179]: time="2024-04-25T08:17:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/52f297415f90f59ff09d5e00b0d0368d8eb4b6e989545c884d880e8fefd9ed50/resolv.conf as [nameserver 10.96.0.10 search minio.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 25 08:28:34 minikube dockerd[940]: time="2024-04-25T08:28:34.457242251Z" level=info msg="ignoring event" container=af5d10399d356cc69984354f5aa5393eb3cfec0b4e697bb00cd608a9f469d1fc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 08:28:34 minikube dockerd[940]: time="2024-04-25T08:28:34.684116865Z" level=info msg="ignoring event" container=52f297415f90f59ff09d5e00b0d0368d8eb4b6e989545c884d880e8fefd9ed50 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 08:30:15 minikube cri-dockerd[1179]: time="2024-04-25T08:30:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/252ceb0221391ce5e0f19d27bd9ccbf27abe4ecfc350e3cb7a3785a4ae547db3/resolv.conf as [nameserver 10.96.0.10 search minio.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 25 08:30:17 minikube cri-dockerd[1179]: time="2024-04-25T08:30:17Z" level=info msg="Stop pulling image minio/minio:latest: Status: Image is up to date for minio/minio:latest"
Apr 25 08:33:01 minikube cri-dockerd[1179]: E0425 08:33:01.380576    1179 httpstream.go:255] error forwarding port 9001 to pod 252ceb0221391ce5e0f19d27bd9ccbf27abe4ecfc350e3cb7a3785a4ae547db3, uid : exit status 1: 2024/04/25 08:33:01 socat[22128] E connect(5, AF=2 127.0.0.1:9001, 16): Connection refused
Apr 25 08:33:01 minikube cri-dockerd[1179]: E0425 08:33:01.385295    1179 httpstream.go:255] error forwarding port 9001 to pod 252ceb0221391ce5e0f19d27bd9ccbf27abe4ecfc350e3cb7a3785a4ae547db3, uid : exit status 1: 2024/04/25 08:33:01 socat[22129] E connect(5, AF=2 127.0.0.1:9001, 16): Connection refused
Apr 25 09:11:10 minikube dockerd[940]: time="2024-04-25T09:11:10.959140905Z" level=info msg="ignoring event" container=f8ad7abf2f4e04043f4b097ddb811437f5e516b91fe5b3d30dab461a0cd210a2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:11:11 minikube dockerd[940]: time="2024-04-25T09:11:11.353010593Z" level=info msg="ignoring event" container=0e23184f3c8828ebe8c0674f2bb5768abba41d651f4643913e73e1e23d041674 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:12:08 minikube dockerd[940]: time="2024-04-25T09:12:08.232193274Z" level=info msg="ignoring event" container=2845ca248620e91d90db915a21e475f4a5cfc8817bb2cfbd13f5a1cda54f511c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:12:08 minikube dockerd[940]: time="2024-04-25T09:12:08.314179069Z" level=info msg="ignoring event" container=d4b9967fa7bd3c3e440440f220164a07f890d5f9eb4e5e2e7852ed39a6291da6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:12:08 minikube dockerd[940]: time="2024-04-25T09:12:08.334981694Z" level=info msg="ignoring event" container=711b261f5a7a52c580cc9c7d63118f5e74da2e6e6ca877e56a927d14d2dc8985 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:12:08 minikube dockerd[940]: time="2024-04-25T09:12:08.527827057Z" level=info msg="ignoring event" container=79939aedd318bcabc9dae4644c407f681967d2a5672a6de3397723fae141d1d2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:12:08 minikube dockerd[940]: time="2024-04-25T09:12:08.643100152Z" level=info msg="ignoring event" container=9d9cab24d727a94a764f1342e9814718ee1001cb0f46e91ab9bdb41b22b22cdc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:12:08 minikube dockerd[940]: time="2024-04-25T09:12:08.855874934Z" level=info msg="ignoring event" container=252ceb0221391ce5e0f19d27bd9ccbf27abe4ecfc350e3cb7a3785a4ae547db3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:12:09 minikube cri-dockerd[1179]: time="2024-04-25T09:12:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5dda652699bfc8c669970327125b1ba1215e1e5ce51691802fc808055fb432b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 25 09:12:09 minikube dockerd[940]: time="2024-04-25T09:12:09.746049712Z" level=info msg="ignoring event" container=dd498b0d1c56bd815042dbe484be8539a47af2e87b773cbcb1d4d82dbade5f66 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:12:10 minikube dockerd[940]: time="2024-04-25T09:12:10.544709650Z" level=info msg="ignoring event" container=e5dda652699bfc8c669970327125b1ba1215e1e5ce51691802fc808055fb432b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:12:12 minikube dockerd[940]: time="2024-04-25T09:12:12.706470234Z" level=info msg="ignoring event" container=dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 25 09:12:12 minikube dockerd[940]: time="2024-04-25T09:12:12.899682032Z" level=info msg="ignoring event" container=708a2fd49a80d6842a1d9b6cc2bcf0d844a8407f7133e97753724b580767d7ef module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
391d1043e3660       10baa1ca17068       About an hour ago   Running             kube-controller-manager   17                  851c1dbc39e9e       kube-controller-manager-minikube
5f39d7a33a854       5374347291230       About an hour ago   Running             kube-apiserver            16                  582f589efd361       kube-apiserver-minikube
d9cc981a7ff47       73deb9a3f7025       About an hour ago   Running             etcd                      17                  b3df6726e86ae       etcd-minikube
2a52adfa86b4b       6d1b4fd1b182d       About an hour ago   Running             kube-scheduler            17                  5fe50bffa5a75       kube-scheduler-minikube
737ac849a6860       73deb9a3f7025       3 hours ago         Exited              etcd                      16                  b6413662122de       etcd-minikube
75a40ef3aac66       6d1b4fd1b182d       3 hours ago         Exited              kube-scheduler            16                  35d496e648889       kube-scheduler-minikube
9922e13294b23       5374347291230       3 hours ago         Exited              kube-apiserver            15                  57f6ab278e73b       kube-apiserver-minikube
11618bb335aa2       10baa1ca17068       3 hours ago         Exited              kube-controller-manager   16                  0f553fd8bdb95       kube-controller-manager-minikube

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_04_19T17_22_28_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 19 Apr 2024 10:22:24 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 25 Apr 2024 09:22:29 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 25 Apr 2024 09:20:59 +0000   Fri, 19 Apr 2024 10:22:22 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 25 Apr 2024 09:20:59 +0000   Fri, 19 Apr 2024 10:22:22 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 25 Apr 2024 09:20:59 +0000   Fri, 19 Apr 2024 10:22:22 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 25 Apr 2024 09:20:59 +0000   Fri, 19 Apr 2024 10:22:29 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             6006856Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             6006856Ki
  pods:               110
System Info:
  Machine ID:                 3e218f053d0849778e1df053783069f9
  System UUID:                3e218f053d0849778e1df053783069f9
  Boot ID:                    deec6f8a-5f16-41da-aad1-45897c21e6fa
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (4 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         10m
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                650m (8%!)(MISSING)   0 (0%!)(MISSING)
  memory             100Mi (1%!)(MISSING)  0 (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.850017] /sbin/ldconfig: 
[  +0.000003] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.019462] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001360] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.001188] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.002423] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.003567] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000894] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.008297] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Saigon not found. Is the tzdata package installed?
[  +0.141649] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001017] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000703] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000866] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001385] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000793] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000685] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002286] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.006674] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.381214] netlink: 'init': attribute type 4 has an invalid length.
[  +0.308151] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.
[  +0.547985] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000074] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.832666] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001211] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000673] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001206] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000946] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000959] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001287] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000337] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001000] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000733] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001178] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000001] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001857] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001161] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000779] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000936] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000770] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000860] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000772] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000759] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.620601] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.006092] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Apr25 08:12] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000012] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.007131] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000007] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.009009] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000009] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.009593] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000007] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.562759] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000007] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.579918] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000006] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.

* 
* ==> etcd [737ac849a686] <==
* {"level":"warn","ts":"2024-04-25T08:03:08.193181Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"774.970705ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-04-25T08:03:08.195175Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"901.675551ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/postgres/postgresdata-postgres-0\" ","response":"range_response_count:1 size:1245"}
{"level":"info","ts":"2024-04-25T08:03:08.195274Z","caller":"traceutil/trace.go:171","msg":"trace[1000751337] range","detail":"{range_begin:/registry/persistentvolumeclaims/postgres/postgresdata-postgres-0; range_end:; response_count:1; response_revision:98527; }","duration":"901.776851ms","start":"2024-04-25T08:03:07.293474Z","end":"2024-04-25T08:03:08.195251Z","steps":["trace[1000751337] 'agreement among raft nodes before linearized reading'  (duration: 271.341527ms)","trace[1000751337] 'range keys from in-memory index tree'  (duration: 630.296824ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-25T08:03:08.195326Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:07.293462Z","time spent":"901.847952ms","remote":"127.0.0.1:54584","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1269,"request content":"key:\"/registry/persistentvolumeclaims/postgres/postgresdata-postgres-0\" "}
{"level":"info","ts":"2024-04-25T08:03:08.246406Z","caller":"traceutil/trace.go:171","msg":"trace[524932945] range","detail":"{range_begin:/registry/podtemplates/; range_end:/registry/podtemplates0; response_count:0; response_revision:98527; }","duration":"775.372205ms","start":"2024-04-25T08:03:07.417845Z","end":"2024-04-25T08:03:08.193217Z","steps":["trace[524932945] 'agreement among raft nodes before linearized reading'  (duration: 146.985885ms)","trace[524932945] 'count revisions from in-memory index tree'  (duration: 627.96412ms)"],"step_count":2}
{"level":"info","ts":"2024-04-25T08:03:08.192877Z","caller":"traceutil/trace.go:171","msg":"trace[116115398] range","detail":"{range_begin:/registry/persistentvolumes/pvc-1b420654-41ae-4f4a-9ba9-a630b4ab6c45; range_end:; response_count:1; response_revision:98527; }","duration":"899.646748ms","start":"2024-04-25T08:03:07.293217Z","end":"2024-04-25T08:03:08.192863Z","steps":["trace[116115398] 'agreement among raft nodes before linearized reading'  (duration: 271.576828ms)","trace[116115398] 'range keys from in-memory index tree'  (duration: 627.98412ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-25T08:03:08.24655Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:07.293194Z","time spent":"953.330252ms","remote":"127.0.0.1:54582","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1232,"request content":"key:\"/registry/persistentvolumes/pvc-1b420654-41ae-4f4a-9ba9-a630b4ab6c45\" "}
{"level":"warn","ts":"2024-04-25T08:03:08.246551Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:07.417833Z","time spent":"828.687309ms","remote":"127.0.0.1:54570","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":0,"response size":30,"request content":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true "}
{"level":"info","ts":"2024-04-25T08:03:08.526925Z","caller":"traceutil/trace.go:171","msg":"trace[1340350970] transaction","detail":"{read_only:false; response_revision:98528; number_of_response:1; }","duration":"329.865141ms","start":"2024-04-25T08:03:08.197045Z","end":"2024-04-25T08:03:08.526911Z","steps":["trace[1340350970] 'process raft request'  (duration: 329.66074ms)"],"step_count":1}
{"level":"info","ts":"2024-04-25T08:03:08.526837Z","caller":"traceutil/trace.go:171","msg":"trace[1204960820] linearizableReadLoop","detail":"{readStateIndex:122607; appliedIndex:122606; }","duration":"328.646438ms","start":"2024-04-25T08:03:08.198174Z","end":"2024-04-25T08:03:08.52682Z","steps":["trace[1204960820] 'read index received'  (duration: 328.622438ms)","trace[1204960820] 'applied index is now lower than readState.Index'  (duration: 23.3µs)"],"step_count":2}
{"level":"warn","ts":"2024-04-25T08:03:08.526994Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"328.829339ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-d071a46d-8d4f-431e-ac80-e65f56846b7d\" ","response":"range_response_count:1 size:1208"}
{"level":"warn","ts":"2024-04-25T08:03:08.52702Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"328.867139ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-8ae5d661-4c2c-48dc-bc50-528081eb1a84\" ","response":"range_response_count:1 size:1236"}
{"level":"info","ts":"2024-04-25T08:03:08.527031Z","caller":"traceutil/trace.go:171","msg":"trace[1487154458] range","detail":"{range_begin:/registry/persistentvolumes/pvc-d071a46d-8d4f-431e-ac80-e65f56846b7d; range_end:; response_count:1; response_revision:98528; }","duration":"328.877539ms","start":"2024-04-25T08:03:08.198143Z","end":"2024-04-25T08:03:08.527021Z","steps":["trace[1487154458] 'agreement among raft nodes before linearized reading'  (duration: 328.791739ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-25T08:03:08.527059Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:08.19813Z","time spent":"328.920439ms","remote":"127.0.0.1:54582","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1232,"request content":"key:\"/registry/persistentvolumes/pvc-d071a46d-8d4f-431e-ac80-e65f56846b7d\" "}
{"level":"warn","ts":"2024-04-25T08:03:08.527067Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"328.899539ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-cc519b51-26df-47e0-9454-bf2887a333fc\" ","response":"range_response_count:1 size:1208"}
{"level":"info","ts":"2024-04-25T08:03:08.527106Z","caller":"traceutil/trace.go:171","msg":"trace[1086069128] range","detail":"{range_begin:/registry/persistentvolumes/pvc-cc519b51-26df-47e0-9454-bf2887a333fc; range_end:; response_count:1; response_revision:98528; }","duration":"328.941039ms","start":"2024-04-25T08:03:08.198156Z","end":"2024-04-25T08:03:08.527097Z","steps":["trace[1086069128] 'agreement among raft nodes before linearized reading'  (duration: 328.869239ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-25T08:03:08.527134Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:08.19815Z","time spent":"328.975939ms","remote":"127.0.0.1:54582","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1232,"request content":"key:\"/registry/persistentvolumes/pvc-cc519b51-26df-47e0-9454-bf2887a333fc\" "}
{"level":"info","ts":"2024-04-25T08:03:08.52705Z","caller":"traceutil/trace.go:171","msg":"trace[1641517288] range","detail":"{range_begin:/registry/persistentvolumes/pvc-8ae5d661-4c2c-48dc-bc50-528081eb1a84; range_end:; response_count:1; response_revision:98528; }","duration":"328.898139ms","start":"2024-04-25T08:03:08.198143Z","end":"2024-04-25T08:03:08.527042Z","steps":["trace[1641517288] 'agreement among raft nodes before linearized reading'  (duration: 328.840039ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-25T08:03:08.527183Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:08.198133Z","time spent":"329.039239ms","remote":"127.0.0.1:54582","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1260,"request content":"key:\"/registry/persistentvolumes/pvc-8ae5d661-4c2c-48dc-bc50-528081eb1a84\" "}
{"level":"warn","ts":"2024-04-25T08:03:08.527204Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:08.19703Z","time spent":"329.960041ms","remote":"127.0.0.1:54588","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:98527 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-04-25T08:03:09.253686Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"926.6919ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-25T08:03:09.253763Z","caller":"traceutil/trace.go:171","msg":"trace[413715301] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:98528; }","duration":"926.781ms","start":"2024-04-25T08:03:08.326968Z","end":"2024-04-25T08:03:09.253749Z","steps":["trace[413715301] 'agreement among raft nodes before linearized reading'  (duration: 200.230689ms)","trace[413715301] 'range keys from in-memory index tree'  (duration: 726.442611ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-25T08:03:09.25382Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:08.326951Z","time spent":"926.8581ms","remote":"127.0.0.1:33060","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-04-25T08:03:09.253945Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"450.570475ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/\" range_end:\"/registry/replicasets0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-04-25T08:03:09.253985Z","caller":"traceutil/trace.go:171","msg":"trace[943073855] range","detail":"{range_begin:/registry/replicasets/; range_end:/registry/replicasets0; response_count:0; response_revision:98528; }","duration":"450.611375ms","start":"2024-04-25T08:03:08.803359Z","end":"2024-04-25T08:03:09.25397Z","steps":["trace[943073855] 'count revisions from in-memory index tree'  (duration: 450.452775ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-25T08:03:09.254016Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:08.803344Z","time spent":"450.662675ms","remote":"127.0.0.1:54904","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":1,"response size":32,"request content":"key:\"/registry/replicasets/\" range_end:\"/registry/replicasets0\" count_only:true "}
{"level":"info","ts":"2024-04-25T08:03:10.78342Z","caller":"traceutil/trace.go:171","msg":"trace[1428197384] transaction","detail":"{read_only:false; response_revision:98529; number_of_response:1; }","duration":"248.848483ms","start":"2024-04-25T08:03:10.534552Z","end":"2024-04-25T08:03:10.783401Z","steps":["trace[1428197384] 'process raft request'  (duration: 248.640983ms)"],"step_count":1}
{"level":"info","ts":"2024-04-25T08:03:31.511974Z","caller":"traceutil/trace.go:171","msg":"trace[1277791857] transaction","detail":"{read_only:false; response_revision:98545; number_of_response:1; }","duration":"562.762058ms","start":"2024-04-25T08:03:30.949192Z","end":"2024-04-25T08:03:31.511954Z","steps":["trace[1277791857] 'process raft request'  (duration: 562.503455ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-25T08:03:31.512345Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:30.949176Z","time spent":"562.919859ms","remote":"127.0.0.1:54588","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:98544 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-04-25T08:03:31.999995Z","caller":"traceutil/trace.go:171","msg":"trace[966699590] linearizableReadLoop","detail":"{readStateIndex:122629; appliedIndex:122628; }","duration":"124.077154ms","start":"2024-04-25T08:03:31.875893Z","end":"2024-04-25T08:03:31.99997Z","steps":["trace[966699590] 'read index received'  (duration: 80.230876ms)","trace[966699590] 'applied index is now lower than readState.Index'  (duration: 43.845178ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-25T08:03:32.000173Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.280856ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:136"}
{"level":"info","ts":"2024-04-25T08:03:31.999998Z","caller":"traceutil/trace.go:171","msg":"trace[126661170] transaction","detail":"{read_only:false; response_revision:98546; number_of_response:1; }","duration":"319.231283ms","start":"2024-04-25T08:03:31.680745Z","end":"2024-04-25T08:03:31.999976Z","steps":["trace[126661170] 'process raft request'  (duration: 275.483606ms)","trace[126661170] 'compare'  (duration: 43.564475ms)"],"step_count":2}
{"level":"info","ts":"2024-04-25T08:03:32.000204Z","caller":"traceutil/trace.go:171","msg":"trace[774174523] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:98546; }","duration":"124.326456ms","start":"2024-04-25T08:03:31.875868Z","end":"2024-04-25T08:03:32.000195Z","steps":["trace[774174523] 'agreement among raft nodes before linearized reading'  (duration: 124.235155ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-25T08:03:32.000284Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:31.680727Z","time spent":"319.492486ms","remote":"127.0.0.1:54684","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:98538 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2024-04-25T08:03:33.435537Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"418.135362ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028743477834553 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc8f13fb293338>","response":"size:42"}
{"level":"info","ts":"2024-04-25T08:03:33.435666Z","caller":"traceutil/trace.go:171","msg":"trace[1673791830] linearizableReadLoop","detail":"{readStateIndex:122630; appliedIndex:122629; }","duration":"1.165527216s","start":"2024-04-25T08:03:32.270126Z","end":"2024-04-25T08:03:33.435653Z","steps":["trace[1673791830] 'read index received'  (duration: 24.401µs)","trace[1673791830] 'applied index is now lower than readState.Index'  (duration: 1.165501115s)"],"step_count":2}
{"level":"warn","ts":"2024-04-25T08:03:33.435718Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:32.001096Z","time spent":"1.434617252s","remote":"127.0.0.1:54456","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-04-25T08:03:33.435795Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.165645917s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-04-25T08:03:33.435802Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"995.127456ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-04-25T08:03:33.435832Z","caller":"traceutil/trace.go:171","msg":"trace[830827986] range","detail":"{range_begin:/registry/minions/; range_end:/registry/minions0; response_count:0; response_revision:98546; }","duration":"995.193257ms","start":"2024-04-25T08:03:32.440631Z","end":"2024-04-25T08:03:33.435824Z","steps":["trace[830827986] 'agreement among raft nodes before linearized reading'  (duration: 995.106056ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-25T08:03:33.435863Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:32.440618Z","time spent":"995.237658ms","remote":"127.0.0.1:54604","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":1,"response size":32,"request content":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true "}
{"level":"info","ts":"2024-04-25T08:03:33.435845Z","caller":"traceutil/trace.go:171","msg":"trace[801112677] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:98546; }","duration":"1.165719618s","start":"2024-04-25T08:03:32.270095Z","end":"2024-04-25T08:03:33.435815Z","steps":["trace[801112677] 'agreement among raft nodes before linearized reading'  (duration: 1.165619417s)"],"step_count":1}
{"level":"warn","ts":"2024-04-25T08:03:33.468914Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-25T08:03:32.270077Z","time spent":"1.198796279s","remote":"127.0.0.1:33060","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-04-25T08:03:33.568365Z","caller":"traceutil/trace.go:171","msg":"trace[1258695628] transaction","detail":"{read_only:false; response_revision:98547; number_of_response:1; }","duration":"131.674937ms","start":"2024-04-25T08:03:33.436671Z","end":"2024-04-25T08:03:33.568346Z","steps":["trace[1258695628] 'process raft request'  (duration: 131.499935ms)"],"step_count":1}
{"level":"info","ts":"2024-04-25T08:04:38.818145Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":98358}
{"level":"info","ts":"2024-04-25T08:04:38.819321Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":98358,"took":"1.033707ms","hash":3271727454}
{"level":"info","ts":"2024-04-25T08:04:38.819364Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3271727454,"revision":98358,"compact-revision":98113}
{"level":"info","ts":"2024-04-25T08:09:38.822044Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":98601}
{"level":"info","ts":"2024-04-25T08:09:38.823143Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":98601,"took":"856.707µs","hash":1519749890}
{"level":"info","ts":"2024-04-25T08:09:38.82319Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1519749890,"revision":98601,"compact-revision":98358}
{"level":"info","ts":"2024-04-25T08:12:17.024385Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-04-25T08:12:17.027344Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-04-25T08:12:17.121287Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-25T08:12:17.122475Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-25T08:12:17.321545Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-25T08:12:17.321615Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-04-25T08:12:17.322522Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-04-25T08:12:17.332904Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-25T08:12:17.333621Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-25T08:12:17.333714Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [d9cc981a7ff4] <==
* {"level":"info","ts":"2024-04-25T08:14:33.947449Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-25T08:14:33.947685Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-25T08:14:33.947716Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-25T08:14:33.952246Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-04-25T08:14:33.952499Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-25T08:14:33.952576Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-25T08:14:33.952663Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-04-25T08:14:33.952723Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-04-25T08:14:34.534773Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 18"}
{"level":"info","ts":"2024-04-25T08:14:34.534877Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 18"}
{"level":"info","ts":"2024-04-25T08:14:34.534906Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 18"}
{"level":"info","ts":"2024-04-25T08:14:34.534928Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 19"}
{"level":"info","ts":"2024-04-25T08:14:34.534939Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 19"}
{"level":"info","ts":"2024-04-25T08:14:34.534958Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 19"}
{"level":"info","ts":"2024-04-25T08:14:34.535042Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 19"}
{"level":"info","ts":"2024-04-25T08:14:34.541992Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-04-25T08:14:34.542042Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-25T08:14:34.542032Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-25T08:14:34.543449Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-04-25T08:14:34.543521Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-04-25T08:14:34.546979Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-04-25T08:14:34.548157Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-04-25T08:24:34.57972Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":99379}
{"level":"info","ts":"2024-04-25T08:24:34.595536Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":99379,"took":"15.507972ms","hash":75131902}
{"level":"info","ts":"2024-04-25T08:24:34.595622Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":75131902,"revision":99379,"compact-revision":98601}
{"level":"info","ts":"2024-04-25T08:29:34.585203Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":99663}
{"level":"info","ts":"2024-04-25T08:29:34.586102Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":99663,"took":"703.905µs","hash":3508612599}
{"level":"info","ts":"2024-04-25T08:29:34.586144Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3508612599,"revision":99663,"compact-revision":99379}
{"level":"info","ts":"2024-04-25T08:34:34.589994Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":99980}
{"level":"info","ts":"2024-04-25T08:34:34.591017Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":99980,"took":"794.703µs","hash":1587007264}
{"level":"info","ts":"2024-04-25T08:34:34.59106Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1587007264,"revision":99980,"compact-revision":99663}
{"level":"warn","ts":"2024-04-25T08:35:03.53156Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.578759ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028744935925194 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:100302 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2024-04-25T08:35:03.531851Z","caller":"traceutil/trace.go:171","msg":"trace[685022891] transaction","detail":"{read_only:false; response_revision:100306; number_of_response:1; }","duration":"177.879929ms","start":"2024-04-25T08:35:03.353944Z","end":"2024-04-25T08:35:03.531825Z","steps":["trace[685022891] 'process raft request'  (duration: 70.016203ms)","trace[685022891] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1091; } (duration: 100.716098ms)"],"step_count":2}
{"level":"info","ts":"2024-04-25T08:39:34.595833Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":100279}
{"level":"info","ts":"2024-04-25T08:39:34.597194Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":100279,"took":"1.095099ms","hash":2564699791}
{"level":"info","ts":"2024-04-25T08:39:34.597237Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2564699791,"revision":100279,"compact-revision":99980}
{"level":"info","ts":"2024-04-25T08:44:34.600004Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":100521}
{"level":"info","ts":"2024-04-25T08:44:34.601101Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":100521,"took":"875.602µs","hash":3875827652}
{"level":"info","ts":"2024-04-25T08:44:34.601174Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3875827652,"revision":100521,"compact-revision":100279}
{"level":"info","ts":"2024-04-25T08:49:34.603818Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":100767}
{"level":"info","ts":"2024-04-25T08:49:34.605142Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":100767,"took":"931.307µs","hash":4151279728}
{"level":"info","ts":"2024-04-25T08:49:34.605188Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4151279728,"revision":100767,"compact-revision":100521}
{"level":"info","ts":"2024-04-25T08:54:34.61084Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":101008}
{"level":"info","ts":"2024-04-25T08:54:34.613583Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":101008,"took":"2.2641ms","hash":1546258368}
{"level":"info","ts":"2024-04-25T08:54:34.613651Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1546258368,"revision":101008,"compact-revision":100767}
{"level":"info","ts":"2024-04-25T08:59:34.616742Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":101252}
{"level":"info","ts":"2024-04-25T08:59:34.6187Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":101252,"took":"1.572512ms","hash":3225827887}
{"level":"info","ts":"2024-04-25T08:59:34.618781Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3225827887,"revision":101252,"compact-revision":101008}
{"level":"info","ts":"2024-04-25T09:04:34.622781Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":101502}
{"level":"info","ts":"2024-04-25T09:04:34.623674Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":101502,"took":"618.132µs","hash":3200820992}
{"level":"info","ts":"2024-04-25T09:04:34.623716Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3200820992,"revision":101502,"compact-revision":101252}
{"level":"info","ts":"2024-04-25T09:09:34.629362Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":101744}
{"level":"info","ts":"2024-04-25T09:09:34.632172Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":101744,"took":"2.197121ms","hash":3031994531}
{"level":"info","ts":"2024-04-25T09:09:34.632301Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3031994531,"revision":101744,"compact-revision":101502}
{"level":"info","ts":"2024-04-25T09:14:34.639855Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":101986}
{"level":"info","ts":"2024-04-25T09:14:34.642002Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":101986,"took":"1.570009ms","hash":3720642483}
{"level":"info","ts":"2024-04-25T09:14:34.642088Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3720642483,"revision":101986,"compact-revision":101744}
{"level":"info","ts":"2024-04-25T09:19:34.644529Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":102310}
{"level":"info","ts":"2024-04-25T09:19:34.646992Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":102310,"took":"2.192717ms","hash":4043014137}
{"level":"info","ts":"2024-04-25T09:19:34.647051Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4043014137,"revision":102310,"compact-revision":101986}

* 
* ==> kernel <==
*  09:22:29 up  6:12,  0 users,  load average: 0.18, 0.22, 0.28
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [5f39d7a33a85] <==
* I0425 08:14:36.404287       1 controller.go:78] Starting OpenAPI AggregationController
I0425 08:14:36.404682       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0425 08:14:36.404736       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0425 08:14:36.404787       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0425 08:14:36.405109       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0425 08:14:36.405179       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0425 08:14:36.405214       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0425 08:14:36.405254       1 available_controller.go:423] Starting AvailableConditionController
I0425 08:14:36.405262       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0425 08:14:36.405272       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0425 08:14:36.405629       1 controller.go:116] Starting legacy_token_tracking_controller
I0425 08:14:36.405676       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0425 08:14:36.405709       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0425 08:14:36.405727       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0425 08:14:36.406134       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0425 08:14:36.406366       1 establishing_controller.go:76] Starting EstablishingController
I0425 08:14:36.405252       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0425 08:14:36.406845       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0425 08:14:36.407035       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0425 08:14:36.407105       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0425 08:14:36.407129       1 crd_finalizer.go:266] Starting CRDFinalizer
I0425 08:14:36.407416       1 controller.go:134] Starting OpenAPI controller
I0425 08:14:36.407493       1 controller.go:85] Starting OpenAPI V3 controller
I0425 08:14:36.407722       1 naming_controller.go:291] Starting NamingConditionController
I0425 08:14:36.409343       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0425 08:14:36.411758       1 aggregator.go:164] waiting for initial CRD sync...
I0425 08:14:36.414927       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0425 08:14:36.414944       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0425 08:14:36.720418       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0425 08:14:36.720878       1 aggregator.go:166] initial CRD sync complete...
I0425 08:14:36.720981       1 autoregister_controller.go:141] Starting autoregister controller
I0425 08:14:36.721017       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0425 08:14:36.742329       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0425 08:14:36.820799       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0425 08:14:36.821005       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0425 08:14:36.821049       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0425 08:14:36.821056       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0425 08:14:36.820971       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0425 08:14:36.822398       1 cache.go:39] Caches are synced for autoregister controller
I0425 08:14:36.821055       1 shared_informer.go:318] Caches are synced for configmaps
I0425 08:14:36.835905       1 shared_informer.go:318] Caches are synced for node_authorizer
I0425 08:14:37.411222       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0425 08:14:38.121630       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0425 08:14:38.123697       1 controller.go:624] quota admission added evaluator for: endpoints
I0425 08:14:38.140270       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0425 08:14:38.929644       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0425 08:14:38.943415       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0425 08:14:39.027862       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0425 08:14:39.067121       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0425 08:14:39.080643       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0425 08:17:50.085790       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0425 08:23:06.637323       1 alloc.go:330] "allocated clusterIPs" service="minio/minio" clusterIPs={"IPv4":"10.103.67.172"}
I0425 08:23:06.649721       1 controller.go:624] quota admission added evaluator for: statefulsets.apps
I0425 08:23:06.656382       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0425 08:29:08.334441       1 controller.go:624] quota admission added evaluator for: namespaces
I0425 08:30:04.272112       1 alloc.go:330] "allocated clusterIPs" service="minio/minio-svc" clusterIPs={"IPv4":"10.111.85.236"}
W0425 09:12:08.411345       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0425 09:12:18.317801       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
I0425 09:14:16.483574       1 alloc.go:330] "allocated clusterIPs" service="postgres/postgres" clusterIPs={"IPv4":"10.108.45.202"}
I0425 09:16:36.560781       1 alloc.go:330] "allocated clusterIPs" service="postgres/postgres-svc" clusterIPs={"IPv4":"10.97.13.52"}

* 
* ==> kube-apiserver [9922e13294b2] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0425 08:12:18.124353       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0425 08:12:18.124401       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0425 08:12:18.124409       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0425 08:12:18.124479       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0425 08:12:18.124866       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0425 08:12:18.126756       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0425 08:12:18.130162       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [11618bb335aa] <==
* E0425 08:08:37.302549       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:08:37.302676       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:08:37.302907       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:08:37.303108       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:08:52.322403       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:08:52.322453       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:08:52.322782       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:08:52.322801       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:09:07.322465       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:09:07.322521       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:09:07.322662       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:09:07.322683       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:09:22.322970       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:09:22.323127       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:09:22.323176       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:09:22.323206       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:09:37.323992       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:09:37.324125       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:09:37.324324       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:09:37.324430       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:09:52.323995       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
E0425 08:09:52.324063       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
I0425 08:09:52.324127       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:09:52.324148       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:10:07.324279       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:10:07.324370       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:10:07.324455       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:10:07.324505       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:10:22.324927       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:10:22.325062       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:10:22.325119       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:10:22.325151       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:10:37.325097       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:10:37.325230       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:10:37.325457       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:10:37.325548       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:10:52.325646       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:10:52.325783       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:10:52.326048       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:10:52.326139       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:11:07.326795       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:11:07.326972       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:11:07.327179       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:11:07.327233       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:11:22.327621       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:11:22.327771       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:11:22.327847       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:11:22.327902       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:11:37.328835       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:11:37.328960       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:11:37.329019       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:11:37.329074       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:11:52.328772       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
I0425 08:11:52.328854       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:11:52.328892       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:11:52.329088       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 08:12:07.329578       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 08:12:07.329642       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 08:12:07.329740       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 08:12:07.329782       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"

* 
* ==> kube-controller-manager [391d1043e366] <==
* I0425 09:12:19.819890       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 09:12:19.819816       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 09:12:34.819947       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 09:12:34.820048       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 09:12:34.820072       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 09:12:34.820088       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 09:12:49.820407       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 09:12:49.820707       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 09:12:49.820834       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
I0425 09:12:49.820879       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 09:13:04.821431       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 09:13:04.821562       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 09:13:04.821797       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 09:13:04.821840       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 09:13:19.821805       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
E0425 09:13:19.821928       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 09:13:19.822137       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 09:13:19.822255       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 09:13:28.523798       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-1-minio-0"
I0425 09:13:28.523868       1 event.go:307] "Event occurred" object="postgres/minio-data-1-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
E0425 09:13:28.526024       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"directpv-min-io\" not found" PVC="postgres/minio-data-2-minio-0"
I0425 09:13:28.526245       1 event.go:307] "Event occurred" object="postgres/minio-data-2-minio-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"directpv-min-io\" not found"
I0425 09:13:38.762328       1 namespace_controller.go:182] "Namespace has been deleted" namespace="postgres"
I0425 09:14:29.310393       1 event.go:307] "Event occurred" object="postgres/postgres" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Claim postgresdata-postgres-0 Pod postgres-0 in StatefulSet postgres success"
I0425 09:14:29.318133       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:14:29.318185       1 event.go:307] "Event occurred" object="postgres/postgres" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod postgres-0 in StatefulSet postgres successful"
I0425 09:14:34.828633       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:14:49.829324       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:15:04.829667       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:15:19.830771       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:15:34.831795       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:15:49.831651       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:16:04.832214       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:16:19.833365       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:16:34.833974       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:16:49.835057       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:17:04.836341       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:17:19.836078       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:17:34.836782       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:17:49.837466       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:18:04.838170       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:18:19.838351       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:18:22.404526       1 stateful_set.go:458] "StatefulSet has been deleted" key="postgres/postgres"
I0425 09:18:28.084653       1 event.go:307] "Event occurred" object="postgres/postgres" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod postgres-0 in StatefulSet postgres successful"
I0425 09:18:34.838958       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:18:49.840138       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:19:04.840572       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:19:19.840630       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:19:34.840793       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:19:49.841204       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:20:04.842496       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:20:19.843488       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:20:34.844470       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:20:49.845193       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:21:04.846317       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:21:19.846643       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:21:34.847493       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:21:49.848470       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:22:04.849172       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0425 09:22:19.849671       1 event.go:307] "Event occurred" object="postgres/postgresdata-postgres-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."

* 
* ==> kube-scheduler [2a52adfa86b4] <==
* I0425 08:14:31.961430       1 serving.go:348] Generated self-signed cert in-memory
W0425 08:14:36.541988       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0425 08:14:36.542228       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0425 08:14:36.542252       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0425 08:14:36.542265       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0425 08:14:36.848170       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0425 08:14:36.848260       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0425 08:14:36.854389       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0425 08:14:36.855460       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0425 08:14:36.857057       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0425 08:14:36.857344       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0425 08:14:36.956098       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0425 08:28:34.399982       1 event_broadcaster.go:265] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minio-0.17c97894f7aad5d1", GenerateName:"", Namespace:"minio", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, EventTime:time.Date(2024, time.April, 25, 8, 28, 34, 396838521, time.Local), Series:(*v1.EventSeries)(nil), ReportingController:"default-scheduler", ReportingInstance:"default-scheduler-minikube", Action:"Scheduling", Reason:"FailedScheduling", Regarding:v1.ObjectReference{Kind:"Pod", Namespace:"minio", Name:"minio-0", UID:"1abc31ab-581f-409f-b5ba-a093ad8bc82e", APIVersion:"v1", ResourceVersion:"99895", FieldPath:""}, Related:(*v1.ObjectReference)(nil), Note:"skip schedule deleting pod: minio/minio-0", Type:"Warning", DeprecatedSource:v1.EventSource{Component:"", Host:""}, DeprecatedFirstTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedLastTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedCount:0}': 'events.events.k8s.io "minio-0.17c97894f7aad5d1" is forbidden: unable to create new content in namespace minio because it is being terminated' (will not retry!)

* 
* ==> kube-scheduler [75a40ef3aac6] <==
* I0425 06:39:37.098129       1 serving.go:348] Generated self-signed cert in-memory
W0425 06:39:40.445083       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0425 06:39:40.445395       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found]
W0425 06:39:40.445435       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0425 06:39:40.445452       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0425 06:39:40.845367       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0425 06:39:40.845856       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0425 06:39:40.854356       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0425 06:39:40.854709       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0425 06:39:40.856875       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0425 06:39:40.857677       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0425 06:39:40.955389       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0425 08:12:16.926995       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0425 08:12:16.929725       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0425 08:12:16.930681       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0425 08:12:16.932674       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Apr 25 09:12:08 minikube kubelet[1609]: I0425 09:12:08.958315    1609 scope.go:117] "RemoveContainer" containerID="711b261f5a7a52c580cc9c7d63118f5e74da2e6e6ca877e56a927d14d2dc8985"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.011895    1609 scope.go:117] "RemoveContainer" containerID="b0b57631b62e369f8c99dc1499ec527690860faa87e7e315679800bce4d7b9e2"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.019786    1609 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"data\" (UniqueName: \"kubernetes.io/host-path/66d6c08f-39cf-4ef5-9dcd-7315aa6a7349-pvc-453a46fb-39d6-41d0-9d94-2a06a3dc7985\") pod \"66d6c08f-39cf-4ef5-9dcd-7315aa6a7349\" (UID: \"66d6c08f-39cf-4ef5-9dcd-7315aa6a7349\") "
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.019838    1609 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-6xmx6\" (UniqueName: \"kubernetes.io/projected/66d6c08f-39cf-4ef5-9dcd-7315aa6a7349-kube-api-access-6xmx6\") pod \"66d6c08f-39cf-4ef5-9dcd-7315aa6a7349\" (UID: \"66d6c08f-39cf-4ef5-9dcd-7315aa6a7349\") "
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.019990    1609 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-zwtnf\" (UniqueName: \"kubernetes.io/projected/b5c241cd-ce27-496e-a91f-a16aa1bf0b35-kube-api-access-zwtnf\") on node \"minikube\" DevicePath \"\""
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.020012    1609 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-t7bht\" (UniqueName: \"kubernetes.io/projected/2212206f-1b60-4bbd-a29a-ecc752fba973-kube-api-access-t7bht\") on node \"minikube\" DevicePath \"\""
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.020027    1609 reconciler_common.go:300] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/b5c241cd-ce27-496e-a91f-a16aa1bf0b35-tmp\") on node \"minikube\" DevicePath \"\""
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.020231    1609 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/66d6c08f-39cf-4ef5-9dcd-7315aa6a7349-pvc-453a46fb-39d6-41d0-9d94-2a06a3dc7985" (OuterVolumeSpecName: "data") pod "66d6c08f-39cf-4ef5-9dcd-7315aa6a7349" (UID: "66d6c08f-39cf-4ef5-9dcd-7315aa6a7349"). InnerVolumeSpecName "pvc-453a46fb-39d6-41d0-9d94-2a06a3dc7985". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.023547    1609 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/66d6c08f-39cf-4ef5-9dcd-7315aa6a7349-kube-api-access-6xmx6" (OuterVolumeSpecName: "kube-api-access-6xmx6") pod "66d6c08f-39cf-4ef5-9dcd-7315aa6a7349" (UID: "66d6c08f-39cf-4ef5-9dcd-7315aa6a7349"). InnerVolumeSpecName "kube-api-access-6xmx6". PluginName "kubernetes.io/projected", VolumeGidValue ""
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.042493    1609 scope.go:117] "RemoveContainer" containerID="711b261f5a7a52c580cc9c7d63118f5e74da2e6e6ca877e56a927d14d2dc8985"
Apr 25 09:12:09 minikube kubelet[1609]: E0425 09:12:09.044333    1609 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 711b261f5a7a52c580cc9c7d63118f5e74da2e6e6ca877e56a927d14d2dc8985" containerID="711b261f5a7a52c580cc9c7d63118f5e74da2e6e6ca877e56a927d14d2dc8985"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.044423    1609 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"711b261f5a7a52c580cc9c7d63118f5e74da2e6e6ca877e56a927d14d2dc8985"} err="failed to get container status \"711b261f5a7a52c580cc9c7d63118f5e74da2e6e6ca877e56a927d14d2dc8985\": rpc error: code = Unknown desc = Error response from daemon: No such container: 711b261f5a7a52c580cc9c7d63118f5e74da2e6e6ca877e56a927d14d2dc8985"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.044444    1609 scope.go:117] "RemoveContainer" containerID="b0b57631b62e369f8c99dc1499ec527690860faa87e7e315679800bce4d7b9e2"
Apr 25 09:12:09 minikube kubelet[1609]: E0425 09:12:09.046410    1609 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: b0b57631b62e369f8c99dc1499ec527690860faa87e7e315679800bce4d7b9e2" containerID="b0b57631b62e369f8c99dc1499ec527690860faa87e7e315679800bce4d7b9e2"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.046479    1609 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"b0b57631b62e369f8c99dc1499ec527690860faa87e7e315679800bce4d7b9e2"} err="failed to get container status \"b0b57631b62e369f8c99dc1499ec527690860faa87e7e315679800bce4d7b9e2\": rpc error: code = Unknown desc = Error response from daemon: No such container: b0b57631b62e369f8c99dc1499ec527690860faa87e7e315679800bce4d7b9e2"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.046498    1609 scope.go:117] "RemoveContainer" containerID="2845ca248620e91d90db915a21e475f4a5cfc8817bb2cfbd13f5a1cda54f511c"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.069680    1609 scope.go:117] "RemoveContainer" containerID="874f1c8965d67a2906dd54a00fc231a3c03f817ece26d57976142d1701e1c063"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.120326    1609 reconciler_common.go:300] "Volume detached for volume \"pvc-453a46fb-39d6-41d0-9d94-2a06a3dc7985\" (UniqueName: \"kubernetes.io/host-path/66d6c08f-39cf-4ef5-9dcd-7315aa6a7349-pvc-453a46fb-39d6-41d0-9d94-2a06a3dc7985\") on node \"minikube\" DevicePath \"\""
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.120371    1609 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-6xmx6\" (UniqueName: \"kubernetes.io/projected/66d6c08f-39cf-4ef5-9dcd-7315aa6a7349-kube-api-access-6xmx6\") on node \"minikube\" DevicePath \"\""
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.137914    1609 scope.go:117] "RemoveContainer" containerID="2845ca248620e91d90db915a21e475f4a5cfc8817bb2cfbd13f5a1cda54f511c"
Apr 25 09:12:09 minikube kubelet[1609]: E0425 09:12:09.139023    1609 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 2845ca248620e91d90db915a21e475f4a5cfc8817bb2cfbd13f5a1cda54f511c" containerID="2845ca248620e91d90db915a21e475f4a5cfc8817bb2cfbd13f5a1cda54f511c"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.139074    1609 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"2845ca248620e91d90db915a21e475f4a5cfc8817bb2cfbd13f5a1cda54f511c"} err="failed to get container status \"2845ca248620e91d90db915a21e475f4a5cfc8817bb2cfbd13f5a1cda54f511c\": rpc error: code = Unknown desc = Error response from daemon: No such container: 2845ca248620e91d90db915a21e475f4a5cfc8817bb2cfbd13f5a1cda54f511c"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.139087    1609 scope.go:117] "RemoveContainer" containerID="874f1c8965d67a2906dd54a00fc231a3c03f817ece26d57976142d1701e1c063"
Apr 25 09:12:09 minikube kubelet[1609]: E0425 09:12:09.140019    1609 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 874f1c8965d67a2906dd54a00fc231a3c03f817ece26d57976142d1701e1c063" containerID="874f1c8965d67a2906dd54a00fc231a3c03f817ece26d57976142d1701e1c063"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.140074    1609 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"874f1c8965d67a2906dd54a00fc231a3c03f817ece26d57976142d1701e1c063"} err="failed to get container status \"874f1c8965d67a2906dd54a00fc231a3c03f817ece26d57976142d1701e1c063\": rpc error: code = Unknown desc = Error response from daemon: No such container: 874f1c8965d67a2906dd54a00fc231a3c03f817ece26d57976142d1701e1c063"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.341391    1609 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e5dda652699bfc8c669970327125b1ba1215e1e5ce51691802fc808055fb432b"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.348647    1609 scope.go:117] "RemoveContainer" containerID="d4b9967fa7bd3c3e440440f220164a07f890d5f9eb4e5e2e7852ed39a6291da6"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.368254    1609 scope.go:117] "RemoveContainer" containerID="d4b9967fa7bd3c3e440440f220164a07f890d5f9eb4e5e2e7852ed39a6291da6"
Apr 25 09:12:09 minikube kubelet[1609]: E0425 09:12:09.369366    1609 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: d4b9967fa7bd3c3e440440f220164a07f890d5f9eb4e5e2e7852ed39a6291da6" containerID="d4b9967fa7bd3c3e440440f220164a07f890d5f9eb4e5e2e7852ed39a6291da6"
Apr 25 09:12:09 minikube kubelet[1609]: I0425 09:12:09.369438    1609 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"d4b9967fa7bd3c3e440440f220164a07f890d5f9eb4e5e2e7852ed39a6291da6"} err="failed to get container status \"d4b9967fa7bd3c3e440440f220164a07f890d5f9eb4e5e2e7852ed39a6291da6\": rpc error: code = Unknown desc = Error response from daemon: No such container: d4b9967fa7bd3c3e440440f220164a07f890d5f9eb4e5e2e7852ed39a6291da6"
Apr 25 09:12:10 minikube kubelet[1609]: I0425 09:12:10.330822    1609 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="2212206f-1b60-4bbd-a29a-ecc752fba973" path="/var/lib/kubelet/pods/2212206f-1b60-4bbd-a29a-ecc752fba973/volumes"
Apr 25 09:12:10 minikube kubelet[1609]: I0425 09:12:10.332625    1609 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="66d6c08f-39cf-4ef5-9dcd-7315aa6a7349" path="/var/lib/kubelet/pods/66d6c08f-39cf-4ef5-9dcd-7315aa6a7349/volumes"
Apr 25 09:12:10 minikube kubelet[1609]: I0425 09:12:10.333247    1609 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="b5c241cd-ce27-496e-a91f-a16aa1bf0b35" path="/var/lib/kubelet/pods/b5c241cd-ce27-496e-a91f-a16aa1bf0b35/volumes"
Apr 25 09:12:10 minikube kubelet[1609]: I0425 09:12:10.730449    1609 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/2eac7ea9-5f49-4ad4-9cc9-affac5731aa7-config-volume\") pod \"2eac7ea9-5f49-4ad4-9cc9-affac5731aa7\" (UID: \"2eac7ea9-5f49-4ad4-9cc9-affac5731aa7\") "
Apr 25 09:12:10 minikube kubelet[1609]: I0425 09:12:10.730847    1609 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-zfhp6\" (UniqueName: \"kubernetes.io/projected/2eac7ea9-5f49-4ad4-9cc9-affac5731aa7-kube-api-access-zfhp6\") pod \"2eac7ea9-5f49-4ad4-9cc9-affac5731aa7\" (UID: \"2eac7ea9-5f49-4ad4-9cc9-affac5731aa7\") "
Apr 25 09:12:10 minikube kubelet[1609]: I0425 09:12:10.731147    1609 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/2eac7ea9-5f49-4ad4-9cc9-affac5731aa7-config-volume" (OuterVolumeSpecName: "config-volume") pod "2eac7ea9-5f49-4ad4-9cc9-affac5731aa7" (UID: "2eac7ea9-5f49-4ad4-9cc9-affac5731aa7"). InnerVolumeSpecName "config-volume". PluginName "kubernetes.io/configmap", VolumeGidValue ""
Apr 25 09:12:10 minikube kubelet[1609]: I0425 09:12:10.733589    1609 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/2eac7ea9-5f49-4ad4-9cc9-affac5731aa7-kube-api-access-zfhp6" (OuterVolumeSpecName: "kube-api-access-zfhp6") pod "2eac7ea9-5f49-4ad4-9cc9-affac5731aa7" (UID: "2eac7ea9-5f49-4ad4-9cc9-affac5731aa7"). InnerVolumeSpecName "kube-api-access-zfhp6". PluginName "kubernetes.io/projected", VolumeGidValue ""
Apr 25 09:12:10 minikube kubelet[1609]: I0425 09:12:10.831169    1609 reconciler_common.go:300] "Volume detached for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/2eac7ea9-5f49-4ad4-9cc9-affac5731aa7-config-volume\") on node \"minikube\" DevicePath \"\""
Apr 25 09:12:10 minikube kubelet[1609]: I0425 09:12:10.831214    1609 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-zfhp6\" (UniqueName: \"kubernetes.io/projected/2eac7ea9-5f49-4ad4-9cc9-affac5731aa7-kube-api-access-zfhp6\") on node \"minikube\" DevicePath \"\""
Apr 25 09:12:11 minikube kubelet[1609]: I0425 09:12:11.401190    1609 scope.go:117] "RemoveContainer" containerID="dd498b0d1c56bd815042dbe484be8539a47af2e87b773cbcb1d4d82dbade5f66"
Apr 25 09:12:12 minikube kubelet[1609]: I0425 09:12:12.334013    1609 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="2eac7ea9-5f49-4ad4-9cc9-affac5731aa7" path="/var/lib/kubelet/pods/2eac7ea9-5f49-4ad4-9cc9-affac5731aa7/volumes"
Apr 25 09:12:12 minikube kubelet[1609]: E0425 09:12:12.736439    1609 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0"
Apr 25 09:12:12 minikube kubelet[1609]: E0425 09:12:12.736553    1609 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-hlhl8_29848126-54b5-4ba7-a62c-9cc3ff6f45c4/coredns/16.log\": failed to reopen container log \"dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-hlhl8_29848126-54b5-4ba7-a62c-9cc3ff6f45c4/coredns/16.log" containerID="dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0"
Apr 25 09:12:12 minikube kubelet[1609]: I0425 09:12:12.946047    1609 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-5pw9m\" (UniqueName: \"kubernetes.io/projected/29848126-54b5-4ba7-a62c-9cc3ff6f45c4-kube-api-access-5pw9m\") pod \"29848126-54b5-4ba7-a62c-9cc3ff6f45c4\" (UID: \"29848126-54b5-4ba7-a62c-9cc3ff6f45c4\") "
Apr 25 09:12:12 minikube kubelet[1609]: I0425 09:12:12.946131    1609 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/29848126-54b5-4ba7-a62c-9cc3ff6f45c4-config-volume\") pod \"29848126-54b5-4ba7-a62c-9cc3ff6f45c4\" (UID: \"29848126-54b5-4ba7-a62c-9cc3ff6f45c4\") "
Apr 25 09:12:12 minikube kubelet[1609]: I0425 09:12:12.946829    1609 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/29848126-54b5-4ba7-a62c-9cc3ff6f45c4-config-volume" (OuterVolumeSpecName: "config-volume") pod "29848126-54b5-4ba7-a62c-9cc3ff6f45c4" (UID: "29848126-54b5-4ba7-a62c-9cc3ff6f45c4"). InnerVolumeSpecName "config-volume". PluginName "kubernetes.io/configmap", VolumeGidValue ""
Apr 25 09:12:12 minikube kubelet[1609]: I0425 09:12:12.948219    1609 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/29848126-54b5-4ba7-a62c-9cc3ff6f45c4-kube-api-access-5pw9m" (OuterVolumeSpecName: "kube-api-access-5pw9m") pod "29848126-54b5-4ba7-a62c-9cc3ff6f45c4" (UID: "29848126-54b5-4ba7-a62c-9cc3ff6f45c4"). InnerVolumeSpecName "kube-api-access-5pw9m". PluginName "kubernetes.io/projected", VolumeGidValue ""
Apr 25 09:12:13 minikube kubelet[1609]: I0425 09:12:13.047028    1609 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-5pw9m\" (UniqueName: \"kubernetes.io/projected/29848126-54b5-4ba7-a62c-9cc3ff6f45c4-kube-api-access-5pw9m\") on node \"minikube\" DevicePath \"\""
Apr 25 09:12:13 minikube kubelet[1609]: I0425 09:12:13.047079    1609 reconciler_common.go:300] "Volume detached for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/29848126-54b5-4ba7-a62c-9cc3ff6f45c4-config-volume\") on node \"minikube\" DevicePath \"\""
Apr 25 09:12:13 minikube kubelet[1609]: I0425 09:12:13.440821    1609 scope.go:117] "RemoveContainer" containerID="dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0"
Apr 25 09:12:13 minikube kubelet[1609]: I0425 09:12:13.470614    1609 scope.go:117] "RemoveContainer" containerID="080c3213aa4047bf57af7d116e967c0f6269c76f2d5fbc19e59992ac64b6607c"
Apr 25 09:12:13 minikube kubelet[1609]: I0425 09:12:13.489383    1609 scope.go:117] "RemoveContainer" containerID="dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0"
Apr 25 09:12:13 minikube kubelet[1609]: E0425 09:12:13.490597    1609 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0" containerID="dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0"
Apr 25 09:12:13 minikube kubelet[1609]: I0425 09:12:13.490668    1609 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0"} err="failed to get container status \"dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0\": rpc error: code = Unknown desc = Error response from daemon: No such container: dd5cd341b4ee2ccb766bcad8aaf34803b62e7652df92a7065e478658612ac4b0"
Apr 25 09:12:13 minikube kubelet[1609]: I0425 09:12:13.490689    1609 scope.go:117] "RemoveContainer" containerID="080c3213aa4047bf57af7d116e967c0f6269c76f2d5fbc19e59992ac64b6607c"
Apr 25 09:12:13 minikube kubelet[1609]: E0425 09:12:13.491817    1609 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 080c3213aa4047bf57af7d116e967c0f6269c76f2d5fbc19e59992ac64b6607c" containerID="080c3213aa4047bf57af7d116e967c0f6269c76f2d5fbc19e59992ac64b6607c"
Apr 25 09:12:13 minikube kubelet[1609]: I0425 09:12:13.491869    1609 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"080c3213aa4047bf57af7d116e967c0f6269c76f2d5fbc19e59992ac64b6607c"} err="failed to get container status \"080c3213aa4047bf57af7d116e967c0f6269c76f2d5fbc19e59992ac64b6607c\": rpc error: code = Unknown desc = Error response from daemon: No such container: 080c3213aa4047bf57af7d116e967c0f6269c76f2d5fbc19e59992ac64b6607c"
Apr 25 09:12:14 minikube kubelet[1609]: I0425 09:12:14.330221    1609 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="29848126-54b5-4ba7-a62c-9cc3ff6f45c4" path="/var/lib/kubelet/pods/29848126-54b5-4ba7-a62c-9cc3ff6f45c4/volumes"
Apr 25 09:14:22 minikube kubelet[1609]: W0425 09:14:22.363898    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 25 09:19:22 minikube kubelet[1609]: W0425 09:19:22.362274    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology

